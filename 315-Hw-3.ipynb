{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Homework 3: In this homework, you will train a two layer neural network using gradient descent. However, instead of manually computing the gradients, you will use the autodiff provided by Tensorflow package."
      ],
      "metadata": {
        "id": "d89fHnC4YR5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "5MLZbUp8aYfE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two-layer Neural Network"
      ],
      "metadata": {
        "id": "tQ6iFPm0Zn8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A two layer neural network contains an input layer, a hidden layer, and an output layer. The number of the input nodes in the diagram below is determined the dimension of our features. We are free to choose the dimension of the hidden layer. As for the final output layer, the number of nodes is determined by the type of the problem. For instance, for a regression problem, we will only have one node and the output value corresponds to our prediction of the target. As for classification, we will first output a vector that has same number of dimension as the number of classes in our classification dataset. Then, we will apply the softmax transformation on the vector to transform real-valued predictions to the class probabilities.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://www.researchgate.net/profile/Haohan-Wang-4/publication/282997080/figure/fig4/AS:305939199610886@1449952997594/A-typical-two-layer-neural-network-Input-layer-does-not-count-as-the-number-of-layers-of.png\" width=\"400\" />\n",
        "</div>\n",
        "\n",
        "\n",
        "Mathematically, this model can be written as\n",
        "\n",
        "$$f(x) =  \\sigma(x^{\\intercal} W_1  +b_1) W_2 + b_2. $$\n",
        "\n",
        "Note that if $x \\in \\mathbb{R}^{d \\times 1}$, then $W_1 \\in \\mathbb{R}^{d \\times H}$ and $W_2 \\in \\mathbb{R}^{H \\times O}$, where $O$ is the output dimension. The dimension of $b_1$ and $b_2$ is self-evident.\n",
        "\n",
        "Given an input $x$, the model first transforms it using the weight matrix $W_1$ and subsequently shifts the output by the bias term $b_1$. The function $\\sigma(.)$ is called the activation function that introdues non-linearity in the model. For the purpose of this homework, we will use the so called relu-activation function that is defined as $\\sigma(t) = \\text{max}\\{t, 0\\}$. Note that that $x^{\\intercal} W_1  +b_1$ generally gives us a vector, so we have to apply the relu activation to each element of the vector. Following the activation, the vector $h(x) =\\sigma(x^{\\intercal} W_1  +b_1)$ is defines the hidden layer. Finally, we apply the linear trasformation $ h(x) W_1 + b_2$ on the hidden layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "SPPcL-_mCZgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This representation of the network is very convenient if you instead want to do matrix operations on your data. Suppose $X$ be your data matrix where $i^{th}$ row of $X$ contains $x_i^{\\intercal}$, then the output of the network on the entire dataset can be written as\n",
        "$$f(X) = \\sigma(X W_1  +b_1) W_2 + b_2. $$"
      ],
      "metadata": {
        "id": "dFuaUzk4WPco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "YKRxewtuYnJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In homework 2, you trained a linear regression model on california housing dataset. In this homework, you will train a two-layer neural network on this dataset using gradient descent."
      ],
      "metadata": {
        "id": "VRXIXnYXGM_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "california_housing = fetch_california_housing( return_X_y=True, as_frame=True)\n",
        "X = california_housing[0]\n",
        "y = california_housing[1]\n",
        "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "sc=StandardScaler()\n",
        "X_train=sc.fit_transform(X_train_unscaled)\n",
        "X_test = sc.transform(X_test_unscaled)"
      ],
      "metadata": {
        "id": "uqhXKxB8equj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting numpy arrays to tensors\n",
        "\n",
        "X_train = tf.convert_to_tensor(X_train, dtype = tf.float32)\n",
        "X_test = tf.convert_to_tensor(X_test, dtype = tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype = tf.float32)\n",
        "y_test= tf.convert_to_tensor(y_test, dtype = tf.float32)"
      ],
      "metadata": {
        "id": "CEjywXczhLNv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)\n",
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oXrgK8_dmcg",
        "outputId": "a5909219-28cb-45c1-bcf7-88d236bc53dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.13350628  0.50935745  0.18106018 ... -0.01082519 -0.8056819\n",
            "   0.78093404]\n",
            " [-0.53221804 -0.6798731  -0.42262954 ... -0.08931585 -1.3394727\n",
            "   1.2452699 ]\n",
            " [ 0.1709897  -0.36274496  0.07312834 ... -0.04480037 -0.49664515\n",
            "  -0.27755183]\n",
            " ...\n",
            " [-0.49478713  0.5886395  -0.59156984 ...  0.01720102 -0.75885814\n",
            "   0.60119116]\n",
            " [ 0.967171   -1.0762833   0.39014888 ...  0.00482125  0.903385\n",
            "  -1.186252  ]\n",
            " [-0.6832017   1.8571521  -0.82965606 ... -0.0816717   0.99235016\n",
            "  -1.4159235 ]], shape=(14448, 8), dtype=float32)\n",
            "tf.Tensor([1.938 1.697 2.598 ... 2.221 2.835 3.25 ], shape=(14448,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 1: Fill in the input dimension and output dimension of the two layer neural network for regression on this dataset. (5 pts) </b></h6></font>\n",
        "For the purpose of this homework, we will just choose hidden layer with dimension double that of input dimension. However, going forward choosing the hidden dimension appropriately would be an important part of deep learning."
      ],
      "metadata": {
        "id": "T2xrmNPqHUvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace ______ with your code\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 2 * input_dim\n",
        "output_dim = 1"
      ],
      "metadata": {
        "id": "byb5Y_HlIKkz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 2: Define a tensorflow variables for weights W1, b1, W2, and b2. Then, initialize both biases b1 and b2 to be 0 vectors and initialize W1 and W2 by picking values uniformly at random from the interval [0,1]. (5 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "KF_mIPodILR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace ______ with your code\n",
        "W1 = tf.Variable(tf.random.uniform(shape=(input_dim, hidden_dim), minval=0, maxval=1))\n",
        "b1 = tf.Variable(tf.zeros(shape=(hidden_dim)))\n",
        "W2 = tf.Variable(tf.random.uniform(shape=(hidden_dim, output_dim), minval=0, maxval=1))\n",
        "b2 = tf.Variable(tf.zeros(shape=(output_dim)))"
      ],
      "metadata": {
        "id": "3TcJV1q0gRpm"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(W1.shape, b1.shape, W2.shape, b2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrNfzNjSDNQm",
        "outputId": "4a001fad-f06b-4cda-aca0-1347918a8262"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 16) (16,) (16, 1) (1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 3: Complete the model function below to define a two layer neural network. Here, inputs is a matrix of shape $n \\times d$, where $i^{th}$ row of the inputs matrix contains $x_i^{\\intercal}$. (10 pts) </b></h6></font>\n",
        "Hint: Use [tf.nn.relu()](https://www.tensorflow.org/api_docs/python/tf/nn/relu) function for relu activation."
      ],
      "metadata": {
        "id": "VuGkzh54JC3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "def model(inputs):\n",
        "  hidden = tf.nn.relu(tf.matmul(inputs, W1) + b1) # hidden layer with relu-activation\n",
        "  output = tf.matmul(hidden, W2) + b2  # fully-connected output linear layer\n",
        "  return output"
      ],
      "metadata": {
        "id": "uD_Rc4AxgS_t"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 4: Write a function below that computes mean-squared error given the predictions and targets. You should use tensorflow operations like tf.square and tf.reduce_mean for autodiff to work. Note that you cannot use inbuilt tensorflow MSE function. (10 pts) </b></h6></font>\n"
      ],
      "metadata": {
        "id": "hzj0hUiiK_DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "def mse(predictions, targets):\n",
        "  squared_error = tf.square(predictions - targets)\n",
        "  mean_squared_error = tf.reduce_mean(squared_error)\n",
        "  return mean_squared_error"
      ],
      "metadata": {
        "id": "1uSkG_ixcLWl"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 5: Complete the function below that takes in features and targets and trains your two layer neural network using gradient descent. Please use autodiff by Gradient taping.  (10 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "v0NCJNXELmK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "learning_rate = 0.01\n",
        "\n",
        "def train(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs)   # predict\n",
        "        loss = mse(predictions, targets)  # loss\n",
        "\n",
        "    # tape gradients of the 4\n",
        "    gradients = tape.gradient(loss, [W1, b1, W2, b2])\n",
        "\n",
        "    # update weights and biases\n",
        "    W1.assign_sub(learning_rate * gradients[0])\n",
        "    b1.assign_sub(learning_rate * gradients[1])\n",
        "    W2.assign_sub(learning_rate * gradients[2])\n",
        "    b2.assign_sub(learning_rate * gradients[3])"
      ],
      "metadata": {
        "id": "9D55l5QAc_Y1"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 6: Complete the routine below that divides the randomly shuffled data into multiple minibatches of size 1000 and use the train function above to run gradient descent on those minibatches. Within each step, you should make a complete pass through the dataset. (10 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "ZCI8zqrL_aFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = 1000\n",
        "n = X_train.shape[0]\n",
        "k = int(n/B)\n",
        "for step in range(100):\n",
        "    X_train = tf.random.shuffle(X_train, seed = step)   #separately shuffling X_train and y_train is reasonable here because of the same seed\n",
        "    y_train = tf.random.shuffle(y_train, seed = step)\n",
        "    #############################################################################\n",
        "    #                             Write your code here                          #\n",
        "    #############################################################################\n",
        "    for i in range(k):\n",
        "      # update batch\n",
        "      start = i * B\n",
        "      end = start + B\n",
        "      X_batch = X_train[start:end , ]\n",
        "      y_batch = y_train[start:end]\n",
        "\n",
        "      # train\n",
        "      train(X_batch, y_batch)\n",
        "\n",
        "    # compute batch loss\n",
        "    loss = mse(model(X_train), y_train)\n",
        "    if (step+1) % 10 == 0:\n",
        "      print(f\"Loss at step {step}: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "gqrMxyB_dkB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c5a87d-1c52-47f4-d336-7035df7448d2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at step 9: 1.4517\n",
            "Loss at step 19: 1.3864\n",
            "Loss at step 29: 1.3674\n",
            "Loss at step 39: 1.3579\n",
            "Loss at step 49: 1.3524\n",
            "Loss at step 59: 1.3491\n",
            "Loss at step 69: 1.3470\n",
            "Loss at step 79: 1.3456\n",
            "Loss at step 89: 1.3446\n",
            "Loss at step 99: 1.3439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 7: Get predictions on your test data set and report the MSE loss on the test data.  (5 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "zHEgstAJMqrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test = model(X_test)\n",
        "test_loss = mse(predictions_test, y_test)\n",
        "print(f\"MSE Loss on the test data: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "MNNF9Sf9lwAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c4f050-9219-4e70-e1e4-cd3dede44b11"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE Loss on the test data: 1.3166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiclass Classification"
      ],
      "metadata": {
        "id": "eO5l34GHZrDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you will train the two layer neural network to do multiclass classification on digits dataset. Digits data is similar to MNIST  but has even smaller pixel."
      ],
      "metadata": {
        "id": "HfaxLYw4XDp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "X,y_int = load_digits(return_X_y=True)\n"
      ],
      "metadata": {
        "id": "PJJii4cQxn_9"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In HW2, you manually created an one hot encoding of the multiclass targets. We can use a function from keras, which is a high level deep learning API thats works pretty well with tensorflow."
      ],
      "metadata": {
        "id": "MqD6t8NRXXRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "y_one_hot = to_categorical(y_int, num_classes=10)"
      ],
      "metadata": {
        "id": "yjf5hOJny-CH"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training-testing split and appropriate rescaling\n",
        "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n",
        "sc=StandardScaler()\n",
        "X_train=sc.fit_transform(X_train_unscaled)\n",
        "X_test = sc.transform(X_test_unscaled)"
      ],
      "metadata": {
        "id": "0zQwnabK0_vW"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to tensors\n",
        "X_train = tf.convert_to_tensor(X_train, dtype = tf.float32)\n",
        "X_test = tf.convert_to_tensor(X_test, dtype = tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype = tf.float32)\n",
        "y_test= tf.convert_to_tensor(y_test, dtype = tf.float32)"
      ],
      "metadata": {
        "id": "I19f0o141D4-"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 8: Fill in the input dimension and output dimension of the two layer neural network appropriate for this dataset. (5 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "98zNQ6TkYMbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = X_train.shape[1] * 2\n",
        "output_dim = y_train.shape[1]"
      ],
      "metadata": {
        "id": "xIUHivlC1Gvd"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 9: Define  tensorflow variables for  weights W1, b1, W2, and b2. Then, initialize both biases b1 and b2 to be 0 and initialize W1 and W2 by picking values uniformly at random from the interval [0, 0.1].  (5 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "ziX9ZaDUYTd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "W1 = tf.Variable(tf.random.uniform(shape=(input_dim, hidden_dim), minval=0, maxval=0.1))\n",
        "b1 = tf.Variable(tf.zeros(shape=(hidden_dim)))\n",
        "W2 = tf.Variable(tf.random.uniform(shape=(hidden_dim, output_dim), minval=0, maxval=0.1))\n",
        "b2 = tf.Variable(tf.zeros(shape=(output_dim)))"
      ],
      "metadata": {
        "id": "2EZe8nmIYTsN"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 10: Complete the classifier function below to define a two layer neural network that outputs class probabilities for each class. Here, inputs is a matrix of shape $n \\times d$, where $i^{th}$ row of the inputs matrix contains $x_i^{\\intercal}$. (10 pts) </b></h6></font> You may use [tf.softmax.nn()](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) function to compute softmax class probabilities."
      ],
      "metadata": {
        "id": "1WF4RjiNZxO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "def classifier(inputs):\n",
        "  hidden = tf.nn.relu(tf.matmul(inputs, W1) + b1) # hidden layer with relu-activation\n",
        "  linear = tf.matmul(hidden, W2) + b2  # fully-connected linear layer\n",
        "  softmax = tf.nn.softmax(linear)  # softmax layer\n",
        "  return softmax\n"
      ],
      "metadata": {
        "id": "hjqk697V1MJW"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 11: Complete the  function below to compute cross entropy loss given one hot encoding of targets and predictions with class probabilities for each prediction. Write the function such that autodiff will work, and note that you cannot use inbuilt tensorflow cross entropy function. (10 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "6Tub82FUbLkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "def cross_entropy(predictions, targets):\n",
        "  cross_entropy = -1 * tf.reduce_sum(targets * tf.math.log(predictions), axis=1)\n",
        "  mean_cross_entropy = tf.reduce_mean(cross_entropy)\n",
        "  return mean_cross_entropy"
      ],
      "metadata": {
        "id": "JHX2NPkw1NWr"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 12: Complete the function below to train the neural network classifier using gradient descent.  (5 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "2EHBjhOnbpmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "learning_rate = 0.1\n",
        "\n",
        "def train(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = classifier(inputs)   # get predictions\n",
        "        loss = cross_entropy(predictions, targets)   # compute loss\n",
        "    gradients = tape.gradient(loss, [W1, b1, W2, b2])   # tape the gradients of both weights and biases\n",
        "\n",
        "    # update all weights and biases using the gradients computed above\n",
        "    W1.assign_sub(learning_rate * gradients[0])\n",
        "    b1.assign_sub(learning_rate * gradients[1])\n",
        "    W2.assign_sub(learning_rate * gradients[2])\n",
        "    b2.assign_sub(learning_rate * gradients[3])\n"
      ],
      "metadata": {
        "id": "yJfNhf4E2wVw"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model using 500 GD iterations."
      ],
      "metadata": {
        "id": "WHU_8kZ8cBlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(500):\n",
        "    train(X_train, y_train)\n",
        "    loss = cross_entropy(classifier(X_train), y_train)\n",
        "    if (step +1)%10==0:\n",
        "      print(f\"Loss at step {step}: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "6PMUUJyA20q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811883a2-49e9-4dd6-8ce8-113cc0e15719"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at step 9: 2.1877\n",
            "Loss at step 19: 2.0467\n",
            "Loss at step 29: 1.8591\n",
            "Loss at step 39: 1.6296\n",
            "Loss at step 49: 1.3826\n",
            "Loss at step 59: 1.1453\n",
            "Loss at step 69: 0.9360\n",
            "Loss at step 79: 0.7646\n",
            "Loss at step 89: 0.6323\n",
            "Loss at step 99: 0.5333\n",
            "Loss at step 109: 0.4591\n",
            "Loss at step 119: 0.4022\n",
            "Loss at step 129: 0.3577\n",
            "Loss at step 139: 0.3218\n",
            "Loss at step 149: 0.2924\n",
            "Loss at step 159: 0.2680\n",
            "Loss at step 169: 0.2474\n",
            "Loss at step 179: 0.2297\n",
            "Loss at step 189: 0.2144\n",
            "Loss at step 199: 0.2010\n",
            "Loss at step 209: 0.1893\n",
            "Loss at step 219: 0.1789\n",
            "Loss at step 229: 0.1696\n",
            "Loss at step 239: 0.1613\n",
            "Loss at step 249: 0.1537\n",
            "Loss at step 259: 0.1467\n",
            "Loss at step 269: 0.1403\n",
            "Loss at step 279: 0.1345\n",
            "Loss at step 289: 0.1291\n",
            "Loss at step 299: 0.1241\n",
            "Loss at step 309: 0.1195\n",
            "Loss at step 319: 0.1151\n",
            "Loss at step 329: 0.1111\n",
            "Loss at step 339: 0.1073\n",
            "Loss at step 349: 0.1037\n",
            "Loss at step 359: 0.1003\n",
            "Loss at step 369: 0.0971\n",
            "Loss at step 379: 0.0941\n",
            "Loss at step 389: 0.0913\n",
            "Loss at step 399: 0.0886\n",
            "Loss at step 409: 0.0860\n",
            "Loss at step 419: 0.0836\n",
            "Loss at step 429: 0.0813\n",
            "Loss at step 439: 0.0791\n",
            "Loss at step 449: 0.0770\n",
            "Loss at step 459: 0.0750\n",
            "Loss at step 469: 0.0730\n",
            "Loss at step 479: 0.0712\n",
            "Loss at step 489: 0.0694\n",
            "Loss at step 499: 0.0676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose, given the vector of class probabilities, you output the label with the highest class probability as your label. As an evaluation of our model, we want to compute the number of mistakes that the model makes. For instance, if $y$ is the true label and $\\widehat{y}$ is the prediction of the model, we will evaluate our model on this point with 0-1 loss\n",
        "$$\\mathbb{1}( \\widehat{y} \\neq y ) = \\begin{cases}1 & \\widehat{y} \\neq y\\\\ 0 & \\widehat{y} = y\\end{cases}$$\n",
        "Over $n$ points, we will compute the mean 0-1 loss,\n",
        "$$\\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}( \\widehat{y}_i \\neq y_i ).$$"
      ],
      "metadata": {
        "id": "9hVr6AALcRMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 13: Compute the mean 0-1 loss of your classifier on the test data. (10 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "N_yAWkZqeV3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labelled(predictions):\n",
        "  max_indices = tf.math.argmax(predictions, axis=1)\n",
        "  output_tensor = tf.one_hot(max_indices, depth=output_dim)\n",
        "  return output_tensor\n",
        "\n",
        "def zero_one_loss(prediction, real):\n",
        "  # is prediction on axis1 all equal to real?\n",
        "  row_equal = tf.reduce_all(tf.equal(prediction, real), axis=1)\n",
        "  # if equal take 0, otherwise 1\n",
        "  loss = 1 - tf.cast(row_equal, tf.int32)\n",
        "  average_loss = tf.reduce_mean(tf.cast(loss, tf.float32))\n",
        "  return average_loss\n",
        "\n",
        "zero_one_loss_val = zero_one_loss(get_labelled(classifier(X_test)), y_test)\n",
        "print(f\"0-1 Loss: {zero_one_loss_val.numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjWunuuxEbCh",
        "outputId": "84b27640-d22c-4be9-fc82-d39d014bca60"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-1 Loss: 0.031481482088565826\n"
          ]
        }
      ]
    }
  ]
}