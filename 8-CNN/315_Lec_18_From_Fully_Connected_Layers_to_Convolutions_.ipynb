{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Notebook credit**: Based on the original D2L notebook [here](https://github.com/d2l-ai/d2l-en-colab/blob/master/chapter_convolutional-neural-networks/why-conv.ipynb)."
      ],
      "metadata": {
        "id": "-LE6lHpIsLpk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "nfUe6zVgsCmL"
      },
      "source": [
        "# From Fully-Connected Layers to Convolutions\n",
        "\n",
        "\n",
        "我们迄今为止讨论过的模型对于 tabular data 是合适的选择。所谓 tabular data, 是指数据由 exmaples 作为行, 与 features 作为列组成。\n",
        "我们很自然会预料到，我们所寻找的 patterns 可能涉及 features 之间的 interaction，但我们并没有 assume any structure *a priori* concerning how the features interact.\n",
        "\n",
        "有时，我们确实缺乏知识来指导构建更 craftier 的 architectures. 在这种情况下, an MLP may be the best that we can do. 然而，对于 high-dimensional perceptual data 来说，这种 structure-less networks 可能会变得越来越笨重。\n",
        "\n",
        "例如，让我们回到区分猫和狗的例子。假设我们在数据收集方面做了充分的工作，收集了一百万像素照片的注释数据集。这意味着网络的每个输入都有一百万个维度。根据我们对全连接层参数化成本的讨论，即使把 hidden dimensions 减少到 1000 个, 也需要 $10^6 \\times 10^3 = 10^9$ 个参数的全连接层。除非我们有大量的 GPU、分布式优化的天赋和超乎寻常的耐心, 否则 learning the parameters of this network 是不可行的。\n",
        "\n",
        "然而，如今人类和计算机都能很好地区分猫和狗，这似乎与上述直觉相矛盾. 这是因为自然图像呈现出丰富的 structures, 使得人类和机器学习模型都可以利用这些结构. Convolutional neural networks (CNNs) 是机器学习利用 natural images 中的一些 known structure 的一种 creative way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invariance(不变量): translation invariance(平移不变性) 与locality(局部性) principle\n",
        "\n",
        "想象一下, 你想要检测图像中的一个物体: 无论我们使用什么方法来识别物体, 我们似乎都不应该过分关注物体在图像中所处的具体位置.\n",
        "猪通常不会飞, 飞机通常不会游. 尽管如此, 如果有一只猪出现在图像的顶部, 我们仍然可以识别出来。CNN 将 *spatial invariance* (空间不变性) 这一理念进行 systematize, 利用它以更少的参数学习有用的 representations (表征).\n",
        "\n",
        "现在, 我们可以通过 enumerate 一些 desiderata 来将这些直觉更加具体化, 以指导我们设计适合计算机视觉的神经网络架构 neural network architecture:\n",
        "\n",
        "1. 在 earliest layers 中, 我们的应该对 the same patch 做出类似的响应, 无论它出现在图像的哪个位置. 这一原则称为 *translation invariance(平移不变性)*.\n",
        "2. network 的 earliest layers 应该聚焦于 local regions, 而不考虑 contents of the image in distant regions. This is the *locality(局部性)* principle. 最终这些 local representations 应当可以被 aggregated to make predictions at the whole image level.\n",
        "\n",
        "Let us see how this translates into mathematics.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s8DaCkzist_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constraining the MLP(多层感知器Multi-Layer Perceptron)\n",
        "\n",
        "MLP 也就是我们之前一直在做的: 把一层层的 neutrons 连接在一起, 每个 neutron 都是对前一层的所有\n",
        "\n",
        "我们可以考虑一个以二维图像 $\\mathbf{X}$ 的形式作为 inputs, 以及它们的 immediate hidden representations 2d tensors $\\mathbf{H}$ 构成的 MLP. 其中 $\\mathbf{X}$ and $\\mathbf{H}$ 大小相同.\n",
        "\n",
        "Let that sink in.\n",
        "We now conceive of not only the inputs but also the hidden representations as possessing spatial structure.\n",
        "\n",
        "我们现在使用 $[\\mathbf{X}]_{i, j}$ and $[\\mathbf{H}]_{i, j}$ 来分别表示 input image 中 ($i$, $j$) 位置处的 pixel 以及它的 hidden representation.\n",
        "\n",
        "因此，为了让每个 hidden unit 接收来自每个 input pixel 的 input, 我们将从使用weight matrix 转为用 **4th-order weight tensors $\\mathsf{W}$** 来表示我们的参数.\n",
        "\n",
        "用 $\\mathbf{U}$ 来表示 biases, 我们可以把这个 fully-connected layer 表达为:\n",
        "\n",
        "$$\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\\end{aligned}$$\n",
        "\n",
        "这是我们熟悉的格式. $[\\mathsf{W}]_{i, j, k, l}$ 表示的是从 $[\\mathbf{X}]_{k, l}$ 在 $\\left[\\mathbf{H}\\right]_{i, j}$ 中的权重. 遍历所有 $k$ 和 $l$, 我们就得到了 $\\left[\\mathbf{H}\\right]_{i, j}$ 的值.\n",
        "\n",
        "但是现在我们换一种思路: 我们对于每个 $(i,j)$ 对, re-index the subscripts $(k, l)$ such that $k = i+a$ and $l = j+b$, 得到一对 $(a,b)$, 其中 $a = k-i$, $b = l-j$.\n",
        "\n",
        "于是我们就把 $[\\mathsf{W}]_{i, j, k, l}$ 转成了 $[\\mathsf{W}]_{i, j, i+a, j+b}$, 我们把它重新写成 $[\\mathsf{V}]_{i, j, a, b} = [\\mathsf{W}]_{i, j, i+a, j+b}$.\n",
        "\n",
        "于是表达式变为:\n",
        "\n",
        "$$\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l} \\\\\n",
        "&= [\\mathbf{U}]_{i, j} +\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}\\end{aligned}$$\n",
        "\n",
        "where the switch from $\\mathsf{W}$ to $\\mathsf{V}$ is entirely cosmetic for now since there is a one-to-one correspondence between coefficients in both fourth-order tensors.\n"
      ],
      "metadata": {
        "id": "8al9DBcHtE0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1)Translation Invariance: 由于平移不变性, $[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}$, 无关于 $i, j$\n",
        "\n",
        "Now let us invoke the first principle\n",
        "established above: translation invariance.\n",
        "This implies that a shift in the input $\\mathbf{X}$\n",
        "should simply lead to a shift in the hidden representation $\\mathbf{H}$.\n",
        "This is only possible if $\\mathsf{V}$ and $\\mathbf{U}$ do not actually depend on $(i, j)$,\n",
        "i.e., we have $[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}$ and $\\mathbf{U}$ is a constant, say $u$.\n",
        "As a result, we can simplify the definition for $\\mathbf{H}$:\n",
        "\n",
        "$$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
        "\n"
      ],
      "metadata": {
        "id": "2qjFEXHItJk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### translation Invariance 使得这个 layer 从 MLP (fully-connected) 变为了一个 *convolution*(严谨称呼其实是cross-correlation)\n",
        "We are effectively weighting pixels at $(i+a, j+b)$\n",
        "in the vicinity of location $(i, j)$ with coefficients $[\\mathbf{V}]_{a, b}$\n",
        "to obtain the value $[\\mathbf{H}]_{i, j}$.\n",
        "Note that $[\\mathbf{V}]_{a, b}$ needs many fewer coefficients than $[\\mathsf{V}]_{i, j, a, b}$ since it\n",
        "no longer depends on the location within the image.\n",
        "We have made significant progress!"
      ],
      "metadata": {
        "id": "C48XEiw1ZVlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###  (3) Locality: (a, b) 不需要遍历全 image, 而是在某个 $\\Delta$ 范围内\n",
        "\n",
        "Now let us invoke the second principle: locality.\n",
        "As motivated above, we believe that we should not have\n",
        "to look very far away from location $(i, j)$\n",
        "in order to glean relevant information\n",
        "to assess what is going on at $[\\mathbf{H}]_{i, j}$.\n",
        "This means that outside some range $|a|> \\Delta$ or $|b| > \\Delta$,\n",
        "we should set $[\\mathbf{V}]_{a, b} = 0$.\n",
        "Equivalently, we can rewrite $[\\mathbf{H}]_{i, j}$ as\n",
        "\n",
        "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
        "\n",
        "实际意义就是: 如果我们要检索一个猫的图像, 那么我们大概在 25x25 的 image 中 设置一个 5x5 的 kernel 就可以了, 这样我们只需要 5x5 的 weight tensor (如果先忽略 channel 的话).\n",
        "\n"
      ],
      "metadata": {
        "id": "SLhke-fUty44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConvoLutional Layer 基本构建完成\n",
        "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
        "于是到此, 我们基本就 build 完了我们的 *convolutional layer*.\n",
        "\n",
        "CNN 就是一种包含 *convolutional layer* 的 special family of NNs.\n",
        "\n",
        "这里的 $\\mathbf{V}$  被称为 *convolution kernel(卷积核)*, 也被称为一个 *filter(滤波器)*, 或者简单地说就是 the layer's *weights*.\n",
        "\n",
        "当 kernel 较小时，与 fully-connected layer 相比, 差异会非常明显. 以前，我们可能需要数十亿个参数来表示图像处理网络中的单层，而现在，我们通常只需要几百个参数，而且不会改变输入或隐藏表示的维度。\n",
        "\n",
        "参数大大幅度减少的代价是: 现在我们只有 translation invariant 的 features, 且我们的 layer 现在只处理了 local information. 所有的 learning 都是依赖于我们的 inductive bias 的. 如果 bias 和现实符合, 那么\n",
        "When that bias agrees with reality, 那么我们就会得到有效的模型. 反之则无法. 比如说, 如果 images 并不是 translation invariant 的, 那么我们的模型甚至不会 fit our training data."
      ],
      "metadata": {
        "id": "pli4SFJEyV-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 什么是一个 ConVolution\n",
        "\n",
        "\n",
        "Before going further, we should briefly review\n",
        "why the above operation is called a convolution.\n",
        "In mathematics, the *convolution* between two functions,\n",
        "say $f, g: \\mathbb{R}^d \\to \\mathbb{R}$ is defined as\n",
        "\n",
        "$$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.$$\n",
        "\n",
        "That is, we measure the overlap between $f$ and $g$\n",
        "when one function is \"flipped\" and shifted by $\\mathbf{x}$.\n",
        "Whenever we have discrete objects, the integral turns into a sum.\n",
        "For instance, for vectors from\n",
        "the set of square summable infinite dimensional vectors\n",
        "with index running over $\\mathbb{Z}$ we obtain the following definition:\n",
        "\n",
        "$$(f * g)(i) = \\sum_a f(a) g(i-a).$$\n",
        "\n",
        "For two-dimensional tensors, we have a corresponding sum\n",
        "with indices $(a, b)$ for $f$ and $(i-a, j-b)$ for $g$, respectively:\n",
        "\n",
        "$$(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b).$$\n",
        "\n",
        "This looks similar to what we had in our convolution layer, with one major difference.\n",
        "Rather than using $(i+a, j+b)$, we are using the difference instead.\n",
        "Our original definition in more properly\n",
        "describes a *cross-correlation*.\n",
        "We will come back to this in the following section.\n",
        "\n"
      ],
      "metadata": {
        "id": "F9h0OQAOuCmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Channels: 考虑到 RGB 三个色道, kernel V 中需要再加一对 channel 与 channel 的坐标对应, 于是变为 4d tensor.\n",
        "\n",
        "\n",
        "There is just one problem with this approach.\n",
        "So far, we blissfully ignored that images consist\n",
        "of 3 channels: red, green, and blue.\n",
        "In reality, images are not two-dimensional objects\n",
        "but rather third-order tensors,\n",
        "characterized by a height, width, and channel,\n",
        "e.g., with shape $1024 \\times 1024 \\times 3$ pixels.\n",
        "While the first two of these axes concern spatial relationships,\n",
        "the third can be regarded as assigning\n",
        "a multidimensional representation to each pixel location.\n",
        "We thus index $\\mathsf{X}$ as $[\\mathsf{X}]_{i, j, k}$.\n",
        "The convolutional filter has to adapt accordingly.\n",
        "Instead of $[\\mathbf{V}]_{a,b}$, we now have $[\\mathsf{V}]_{a,b,c}$.\n",
        "\n",
        "Moreover, just as our input consists of a third-order tensor,\n",
        "it turns out to be a good idea to similarly formulate\n",
        "our hidden representations as third-order tensors $\\mathsf{H}$.\n",
        "In other words, rather than just having a single hidden representation\n",
        "corresponding to each spatial location,\n",
        "we want an entire vector of hidden representations\n",
        "corresponding to each spatial location.\n",
        "We could think of the hidden representations as comprising\n",
        "a number of two-dimensional grids stacked on top of each other.\n",
        "As in the inputs, these are sometimes called *channels*.\n",
        "They are also sometimes called *feature maps*,\n",
        "as each provides a spatialized set\n",
        "of learned features to the subsequent layer.\n",
        "Intuitively, you might imagine that at lower layers that are closer to inputs,\n",
        "some channels could become specialized to recognize edges while\n",
        "others could recognize textures.\n",
        "\n",
        "\n",
        "To support multiple channels in both inputs ($\\mathsf{X}$) and hidden representations ($\\mathsf{H}$),\n",
        "we can add a fourth coordinate to $\\mathsf{V}$: $[\\mathsf{V}]_{a, b, c, d}$.\n",
        "Putting everything together we have:\n",
        "\n",
        "$$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c},$$\n",
        "\n",
        "\n",
        "where $d$ indexes the output channels in the hidden representations $\\mathsf{H}$. The subsequent convolutional layer will go on to take a third-order tensor, $\\mathsf{H}$, as the input.\n",
        "Being more general, this is\n",
        "the definition of a convolutional layer for multiple channels, where $\\mathsf{V}$ is a kernel or filter of the layer.\n",
        "\n",
        "There are still many operations that we need to address.\n",
        "For instance, we need to figure out how to combine all the hidden representations\n",
        "to a single output.\n",
        "We also need to decide how to compute things efficiently,\n",
        "how to combine multiple layers,\n",
        "appropriate activation functions,\n",
        "and how to make reasonable design choices\n",
        "to yield networks that are effective in practice.\n",
        "We turn to these issues in later lectures.\n",
        "\n"
      ],
      "metadata": {
        "id": "iqzhpuMwucAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Summary\n",
        "\n",
        "* Translation invariance in images implies that all patches of an image will be treated in the same manner.\n",
        "* Locality means that only a small neighborhood of pixels will be used to compute the corresponding hidden representations.\n",
        "* In image processing, convolutional layers typically require many fewer parameters than fully-connected layers.\n",
        "* CNNS are a special family of neural networks that contain convolutional layers.\n",
        "* Channels on input and output allow our model to capture multiple aspects of an image  at each spatial location.\n"
      ],
      "metadata": {
        "id": "UjP3M9XkupNt"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}